{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype Documentation\n",
    "Welcome. This is the notebook where you will find the documentation and code of the Model used for the debt analysis that will be rolling out every month. Initially, the model used to be coded in Excel, which despite its power and ease of use, can come short when performing data analysis of big datasets. The Excel model took many hours to develop and the time that you had to invest was not a one-off but rather every time the analysis had to be updated. These limitations can be easily overcome with the help of a few lines of code. Due to its simplicity and readability, Python is one of the biggest players in the market right now. It's open-source and has a very active community that created many of the most used libraries for data analysis. Furthermore, it's the go-to language when you want to incorporate machine learning models due to Tensoflow, Keras, Pytorch and other libraries. \n",
    "\n",
    "We also have to acknowledge that Python is not ideal in certain scenarios due to it's slow computatinal speed in comparission with alternatives such as C++. This speed is due to the way the language is constructued. Some of them are very close ot the machine-language that gets communicated to the CPU, but other have several layers of abstraction that makes for a more understandable syntax for non-programmer humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "We start by importing the necessary libraries for the analysis. A Python library is a collection of related modules. It contains bundles of code that can be used repeatedly in different programs. It makes Python Programming simpler and convenient for the programmer. As we donâ€™t need to write the same code again and again for different programs. Python libraries play a very vital role in fields of Machine Learning, Data Science, Data Visualization, etc. \n",
    "\n",
    "This time, we only need to make use of (insert number here) libraries: Pandas, written for data manipulation and analysis; Numpy, a library which adds support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays; and (insert name here), which allows us to communicate with our data provider's API.\n",
    "\n",
    "You might be wondering, what is an API? Let's explain it with an example. When you use an application on your mobile phone, the application connects to the Internet and sends data to a server. The server then retrieves that data, interprets it, performs the necessary actions and sends it back to your phone. The application then interprets that data and presents you with the information you wanted in a readable way. This is what an API is - all of this happens via API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Structures: Notebooks vs Scripts\n",
    "This notebook has minor differences with the actual code that the model runs. What you are reading right now is Jupyter Notebook, a flexible format that allows us to combine annotations with code blocks. This format is idea when explaining the code step by step but has drawbacks: \n",
    "- *Difficult to experiment*: You may want to test with different methods of processing your data, choose different parameters for your machine learning algorithm to see if the accuracy increases. But every time you experiment with new methods, you need to find and rerun the related cells. This is confusing and time-consuming, especially when the processing procedure or the training takes a long time to run. \n",
    "- *Not ideal for reproducibility*: If you want to use new data with a slightly different structure, it would be difficult to identify the source of error in your notebook.\n",
    "- *Difficult to debug*: When you get an error in your code, it is difficult to know whether the reason for the error is the code or the change in data. If the error is in the code, which part of the code is causing the problem?\n",
    "- *Not ideal for production*: Jupyter Notebook does not play very well with other tools. It is not easy to run the code from Jupyter Notebook while using other tools.\n",
    "\n",
    "Therefore, the execution of the model is done through a script and the documentation is done in a notebook. The script is going to be broadly similar to the notebook and will diverge just in its strucute. It is split into different funcitons that are equivalent to some of the blocks here. At the end, a main function is executed and this main funciton is the one resposible for calling all other funciton sin the right order to produce the desired output. Finally, we save the output to different formats. We can create a SQL database, an Excelfile, a CSV file and many other options. For simplicity and due to the legacy of Excel in the bankign sector, we format the output as an Excel workbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "Now we move on to the actual code behind the model. It's divided in four sections: Data Retrieval, Data Wranlging, Data Visualisation, and Final Formatting.\n",
    "\n",
    "### Data Retrieval\n",
    "First things first, we need to get the data. For this purpose we connect to IHS Markit API and request it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling\n",
    "Now that we have the data, we have to put it all together in one big dataframe. Dataframe is just the way of calling matrices in Python, they are data structures that we can manipulate through different operations. At the end, we will  have one bid matrix with all the information that we want. We will clean the data, add new variables that come out of the interaction and prepare it for the visualisation of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation\n",
    "We want to visualise the data in two different ways: on an aggregate level and individually per country. The former is an ad-hoc code that shows us aggregate informatoin and overall statistics, while the later is simple to execute since we just have to code for one of the countries and then run a loop that goes through each country, repeating the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Formatting\n",
    "At last, we have done everythign but we have to convert it to the right output so that our colleagues can observe the results in a concise and easy-to-understand way. For this we will create an Excel Workbook and display a dashboard that summarises three speadsheets: one spreadsheet with the general database, one spreadsheet for the aggregate statistics and visualisations, and one spreadsheet with filters that let us observe individual contruies and its visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future iterations\n",
    "Even though we have gone through the full explanation of the model and its code, there is plenty of room for imporvement. For example, what if we can create a model that constantly shows a dashboard that automatically updates itself when new information is available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
